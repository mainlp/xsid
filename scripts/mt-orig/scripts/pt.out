Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer_wmt_en_de', attention_dropout=0.0, best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='../preprocessed/pt', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layer_wise_attention=False, layernorm_embedding=False, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format='simple', log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=20, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='../models/pt/checkpoint_last.pt', save_dir='../models/pt', save_interval=1, save_interval_updates=0, seed=1111, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', target_lang='pt', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0)
| [en] dictionary: 32000 types
| [pt] dictionary: 32000 types
| loaded 1958 examples from: ../preprocessed/pt/valid.en-pt.en
| loaded 1958 examples from: ../preprocessed/pt/valid.en-pt.pt
| ../preprocessed/pt valid en-pt 1958 examples
TransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(32000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(32000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
| model transformer_wmt_en_de, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 93290496 (num. trained: 93290496)
| training on 1 GPUs
| max tokens per GPU = 4096 and max sentences per GPU = None
| NOTICE: your device may support faster training with --fp16
| loaded checkpoint ../models/pt/checkpoint_last.pt (epoch 19 @ 1631834 updates)
| loading train data for epoch 19
| loaded 33375963 examples from: ../preprocessed/pt/train.en-pt.en
| loaded 33375963 examples from: ../preprocessed/pt/train.en-pt.pt
| ../preprocessed/pt train en-pt 33375963 examples
| WARNING: 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[16644205]
/home/robv/nlu-mt/fairseq/fairseq/optim/adam.py:160: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
| epoch 020:   1000 / 85886 loss=3.760, nll_loss=2.144, ppl=4.42, wps=17784, ups=5, wpb=3545.142, bsz=398.426, num_updates=1.63284e+06, lr=2.47474e-05, gnorm=3.629, clip=0.002, oom=0.000, wall=496, train_wall=315118
| epoch 020:   2000 / 85886 loss=3.767, nll_loss=2.151, ppl=4.44, wps=18197, ups=5, wpb=3565.791, bsz=389.025, num_updates=1.63384e+06, lr=2.47398e-05, gnorm=3.545, clip=0.001, oom=0.000, wall=688, train_wall=315306
| epoch 020:   3000 / 85886 loss=3.763, nll_loss=2.147, ppl=4.43, wps=18398, ups=5, wpb=3577.463, bsz=391.099, num_updates=1.63484e+06, lr=2.47322e-05, gnorm=3.507, clip=0.001, oom=0.000, wall=880, train_wall=315493
| epoch 020:   4000 / 85886 loss=3.757, nll_loss=2.139, ppl=4.41, wps=18498, ups=5, wpb=3583.138, bsz=389.821, num_updates=1.63584e+06, lr=2.47247e-05, gnorm=3.486, clip=0.001, oom=0.000, wall=1071, train_wall=315681
| epoch 020:   5000 / 85886 loss=3.764, nll_loss=2.147, ppl=4.43, wps=18429, ups=5, wpb=3587.349, bsz=387.988, num_updates=1.63684e+06, lr=2.47171e-05, gnorm=3.483, clip=0.001, oom=0.000, wall=1270, train_wall=315875
| epoch 020:   6000 / 85886 loss=3.759, nll_loss=2.142, ppl=4.41, wps=18367, ups=5, wpb=3586.420, bsz=390.019, num_updates=1.63784e+06, lr=2.47096e-05, gnorm=3.468, clip=0.000, oom=0.000, wall=1468, train_wall=316069
| epoch 020:   7000 / 85886 loss=3.761, nll_loss=2.144, ppl=4.42, wps=18424, ups=5, wpb=3589.556, bsz=388.647, num_updates=1.63884e+06, lr=2.4702e-05, gnorm=3.468, clip=0.001, oom=0.000, wall=1660, train_wall=316258
| epoch 020:   8000 / 85886 loss=3.759, nll_loss=2.141, ppl=4.41, wps=18462, ups=5, wpb=3593.673, bsz=388.208, num_updates=1.63984e+06, lr=2.46945e-05, gnorm=3.460, clip=0.000, oom=0.000, wall=1854, train_wall=316447
| epoch 020:   9000 / 85886 loss=3.758, nll_loss=2.141, ppl=4.41, wps=18353, ups=5, wpb=3589.899, bsz=388.212, num_updates=1.64084e+06, lr=2.4687e-05, gnorm=3.465, clip=0.000, oom=0.000, wall=2057, train_wall=316646
| epoch 020:  10000 / 85886 loss=3.756, nll_loss=2.139, ppl=4.4, wps=18390, ups=5, wpb=3593.122, bsz=387.444, num_updates=1.64184e+06, lr=2.46794e-05, gnorm=3.451, clip=0.000, oom=0.000, wall=2250, train_wall=316835
| epoch 020:  11000 / 85886 loss=3.755, nll_loss=2.137, ppl=4.4, wps=18435, ups=5, wpb=3595.901, bsz=388.355, num_updates=1.64284e+06, lr=2.46719e-05, gnorm=3.449, clip=0.000, oom=0.000, wall=2442, train_wall=317023
| epoch 020:  12000 / 85886 loss=3.755, nll_loss=2.137, ppl=4.4, wps=18403, ups=5, wpb=3598.127, bsz=389.622, num_updates=1.64384e+06, lr=2.46644e-05, gnorm=3.447, clip=0.000, oom=0.000, wall=2643, train_wall=317220
| epoch 020:  13000 / 85886 loss=3.754, nll_loss=2.136, ppl=4.4, wps=18407, ups=5, wpb=3598.975, bsz=389.294, num_updates=1.64484e+06, lr=2.46569e-05, gnorm=3.447, clip=0.000, oom=0.000, wall=2838, train_wall=317411
| epoch 020:  14000 / 85886 loss=3.755, nll_loss=2.137, ppl=4.4, wps=18421, ups=5, wpb=3600.464, bsz=389.368, num_updates=1.64584e+06, lr=2.46494e-05, gnorm=3.446, clip=0.000, oom=0.000, wall=3033, train_wall=317602
| epoch 020:  15000 / 85886 loss=3.757, nll_loss=2.139, ppl=4.41, wps=18423, ups=5, wpb=3601.295, bsz=389.288, num_updates=1.64684e+06, lr=2.46419e-05, gnorm=3.447, clip=0.000, oom=0.000, wall=3229, train_wall=317794
| epoch 020:  16000 / 85886 loss=3.759, nll_loss=2.141, ppl=4.41, wps=18428, ups=5, wpb=3600.790, bsz=388.354, num_updates=1.64784e+06, lr=2.46345e-05, gnorm=3.447, clip=0.000, oom=0.000, wall=3423, train_wall=317984
| epoch 020:  17000 / 85886 loss=3.758, nll_loss=2.140, ppl=4.41, wps=18439, ups=5, wpb=3598.439, bsz=388.211, num_updates=1.64884e+06, lr=2.4627e-05, gnorm=3.457, clip=0.000, oom=0.000, wall=3614, train_wall=318171
| epoch 020:  18000 / 85886 loss=3.758, nll_loss=2.140, ppl=4.41, wps=18454, ups=5, wpb=3596.505, bsz=388.493, num_updates=1.64984e+06, lr=2.46195e-05, gnorm=3.461, clip=0.000, oom=0.000, wall=3805, train_wall=318357
| epoch 020:  19000 / 85886 loss=3.757, nll_loss=2.140, ppl=4.41, wps=18470, ups=5, wpb=3596.347, bsz=388.850, num_updates=1.65084e+06, lr=2.46121e-05, gnorm=3.460, clip=0.000, oom=0.000, wall=3996, train_wall=318545
| epoch 020:  20000 / 85886 loss=3.757, nll_loss=2.139, ppl=4.4, wps=18494, ups=5, wpb=3598.321, bsz=388.927, num_updates=1.65184e+06, lr=2.46046e-05, gnorm=3.459, clip=0.000, oom=0.000, wall=4188, train_wall=318733
| epoch 020:  21000 / 85886 loss=3.756, nll_loss=2.138, ppl=4.4, wps=18506, ups=5, wpb=3597.883, bsz=389.472, num_updates=1.65284e+06, lr=2.45972e-05, gnorm=3.459, clip=0.000, oom=0.000, wall=4379, train_wall=318920
| epoch 020:  22000 / 85886 loss=3.756, nll_loss=2.139, ppl=4.4, wps=18514, ups=5, wpb=3598.364, bsz=389.898, num_updates=1.65384e+06, lr=2.45897e-05, gnorm=3.461, clip=0.000, oom=0.000, wall=4572, train_wall=319109
| epoch 020:  23000 / 85886 loss=3.756, nll_loss=2.138, ppl=4.4, wps=18511, ups=5, wpb=3598.526, bsz=389.816, num_updates=1.65484e+06, lr=2.45823e-05, gnorm=3.462, clip=0.000, oom=0.000, wall=4768, train_wall=319300
| epoch 020:  24000 / 85886 loss=3.756, nll_loss=2.139, ppl=4.4, wps=18505, ups=5, wpb=3599.507, bsz=389.895, num_updates=1.65584e+06, lr=2.45749e-05, gnorm=3.461, clip=0.000, oom=0.000, wall=4965, train_wall=319494
| epoch 020:  25000 / 85886 loss=3.756, nll_loss=2.138, ppl=4.4, wps=18478, ups=5, wpb=3599.301, bsz=389.673, num_updates=1.65684e+06, lr=2.45675e-05, gnorm=3.461, clip=0.000, oom=0.000, wall=5166, train_wall=319691
| epoch 020:  26000 / 85886 loss=3.757, nll_loss=2.139, ppl=4.4, wps=18463, ups=5, wpb=3599.162, bsz=389.443, num_updates=1.65784e+06, lr=2.45601e-05, gnorm=3.462, clip=0.000, oom=0.000, wall=5365, train_wall=319885
| epoch 020:  27000 / 85886 loss=3.757, nll_loss=2.140, ppl=4.41, wps=18451, ups=5, wpb=3598.966, bsz=389.291, num_updates=1.65884e+06, lr=2.45527e-05, gnorm=3.463, clip=0.000, oom=0.000, wall=5563, train_wall=320079
| epoch 020:  28000 / 85886 loss=3.758, nll_loss=2.140, ppl=4.41, wps=18451, ups=5, wpb=3598.572, bsz=389.092, num_updates=1.65984e+06, lr=2.45453e-05, gnorm=3.463, clip=0.000, oom=0.000, wall=5758, train_wall=320270
| epoch 020:  29000 / 85886 loss=3.759, nll_loss=2.142, ppl=4.41, wps=18459, ups=5, wpb=3597.244, bsz=388.817, num_updates=1.66084e+06, lr=2.45379e-05, gnorm=3.464, clip=0.000, oom=0.000, wall=5948, train_wall=320456
| epoch 020:  30000 / 85886 loss=3.760, nll_loss=2.142, ppl=4.41, wps=18469, ups=5, wpb=3596.748, bsz=389.061, num_updates=1.66184e+06, lr=2.45305e-05, gnorm=3.465, clip=0.000, oom=0.000, wall=6139, train_wall=320644
| epoch 020:  31000 / 85886 loss=3.760, nll_loss=2.143, ppl=4.42, wps=18471, ups=5, wpb=3597.981, bsz=388.718, num_updates=1.66284e+06, lr=2.45231e-05, gnorm=3.463, clip=0.000, oom=0.000, wall=6335, train_wall=320836
| epoch 020:  32000 / 85886 loss=3.760, nll_loss=2.143, ppl=4.42, wps=18475, ups=5, wpb=3598.569, bsz=388.912, num_updates=1.66384e+06, lr=2.45157e-05, gnorm=3.462, clip=0.000, oom=0.000, wall=6530, train_wall=321026
| epoch 020:  33000 / 85886 loss=3.760, nll_loss=2.143, ppl=4.42, wps=18473, ups=5, wpb=3598.897, bsz=388.979, num_updates=1.66484e+06, lr=2.45084e-05, gnorm=3.464, clip=0.000, oom=0.000, wall=6726, train_wall=321218
| epoch 020:  34000 / 85886 loss=3.760, nll_loss=2.143, ppl=4.42, wps=18444, ups=5, wpb=3598.628, bsz=389.145, num_updates=1.66584e+06, lr=2.4501e-05, gnorm=3.463, clip=0.000, oom=0.000, wall=6930, train_wall=321419
| epoch 020:  35000 / 85886 loss=3.760, nll_loss=2.143, ppl=4.42, wps=18430, ups=5, wpb=3598.073, bsz=388.766, num_updates=1.66684e+06, lr=2.44937e-05, gnorm=3.462, clip=0.000, oom=0.000, wall=7130, train_wall=321614
| epoch 020:  36000 / 85886 loss=3.760, nll_loss=2.143, ppl=4.42, wps=18437, ups=5, wpb=3598.116, bsz=388.657, num_updates=1.66784e+06, lr=2.44863e-05, gnorm=3.462, clip=0.000, oom=0.000, wall=7322, train_wall=321803
| epoch 020:  37000 / 85886 loss=3.761, nll_loss=2.144, ppl=4.42, wps=18445, ups=5, wpb=3597.822, bsz=388.646, num_updates=1.66884e+06, lr=2.4479e-05, gnorm=3.464, clip=0.000, oom=0.000, wall=7513, train_wall=321990
| epoch 020:  38000 / 85886 loss=3.762, nll_loss=2.145, ppl=4.42, wps=18450, ups=5, wpb=3597.462, bsz=388.537, num_updates=1.66984e+06, lr=2.44716e-05, gnorm=3.463, clip=0.000, oom=0.000, wall=7706, train_wall=322179
| epoch 020:  39000 / 85886 loss=3.762, nll_loss=2.145, ppl=4.42, wps=18435, ups=5, wpb=3597.239, bsz=388.425, num_updates=1.67084e+06, lr=2.44643e-05, gnorm=3.464, clip=0.000, oom=0.000, wall=7907, train_wall=322375
| epoch 020:  40000 / 85886 loss=3.762, nll_loss=2.145, ppl=4.42, wps=18444, ups=5, wpb=3597.370, bsz=388.451, num_updates=1.67184e+06, lr=2.4457e-05, gnorm=3.464, clip=0.000, oom=0.000, wall=8098, train_wall=322563
| epoch 020:  41000 / 85886 loss=3.761, nll_loss=2.144, ppl=4.42, wps=18452, ups=5, wpb=3597.281, bsz=388.504, num_updates=1.67284e+06, lr=2.44497e-05, gnorm=3.463, clip=0.000, oom=0.000, wall=8290, train_wall=322750
| epoch 020:  42000 / 85886 loss=3.761, nll_loss=2.144, ppl=4.42, wps=18447, ups=5, wpb=3596.724, bsz=388.639, num_updates=1.67384e+06, lr=2.44424e-05, gnorm=3.465, clip=0.000, oom=0.000, wall=8486, train_wall=322942
| epoch 020:  43000 / 85886 loss=3.761, nll_loss=2.144, ppl=4.42, wps=18435, ups=5, wpb=3596.848, bsz=389.021, num_updates=1.67484e+06, lr=2.44351e-05, gnorm=3.465, clip=0.000, oom=0.000, wall=8686, train_wall=323138
| epoch 020:  44000 / 85886 loss=3.760, nll_loss=2.143, ppl=4.42, wps=18442, ups=5, wpb=3596.719, bsz=389.123, num_updates=1.67584e+06, lr=2.44278e-05, gnorm=3.465, clip=0.000, oom=0.000, wall=8878, train_wall=323326
| epoch 020:  45000 / 85886 loss=3.761, nll_loss=2.144, ppl=4.42, wps=18446, ups=5, wpb=3596.635, bsz=389.182, num_updates=1.67684e+06, lr=2.44205e-05, gnorm=3.466, clip=0.000, oom=0.000, wall=9071, train_wall=323515
| epoch 020:  46000 / 85886 loss=3.761, nll_loss=2.144, ppl=4.42, wps=18452, ups=5, wpb=3596.139, bsz=388.940, num_updates=1.67784e+06, lr=2.44132e-05, gnorm=3.467, clip=0.000, oom=0.000, wall=9262, train_wall=323702
| epoch 020:  47000 / 85886 loss=3.760, nll_loss=2.143, ppl=4.42, wps=18460, ups=5, wpb=3596.765, bsz=389.048, num_updates=1.67884e+06, lr=2.4406e-05, gnorm=3.467, clip=0.000, oom=0.000, wall=9454, train_wall=323890
| epoch 020:  48000 / 85886 loss=3.760, nll_loss=2.143, ppl=4.42, wps=18443, ups=5, wpb=3596.734, bsz=389.106, num_updates=1.67984e+06, lr=2.43987e-05, gnorm=3.467, clip=0.000, oom=0.000, wall=9658, train_wall=324090
| epoch 020:  49000 / 85886 loss=3.760, nll_loss=2.143, ppl=4.42, wps=18445, ups=5, wpb=3596.504, bsz=389.057, num_updates=1.68084e+06, lr=2.43914e-05, gnorm=3.467, clip=0.000, oom=0.000, wall=9851, train_wall=324279
| epoch 020:  50000 / 85886 loss=3.761, nll_loss=2.144, ppl=4.42, wps=18446, ups=5, wpb=3597.028, bsz=388.785, num_updates=1.68184e+06, lr=2.43842e-05, gnorm=3.466, clip=0.000, oom=0.000, wall=10047, train_wall=324471
| epoch 020:  51000 / 85886 loss=3.760, nll_loss=2.143, ppl=4.42, wps=18450, ups=5, wpb=3597.475, bsz=388.963, num_updates=1.68284e+06, lr=2.43769e-05, gnorm=3.466, clip=0.000, oom=0.000, wall=10241, train_wall=324661
| epoch 020:  52000 / 85886 loss=3.761, nll_loss=2.143, ppl=4.42, wps=18440, ups=5, wpb=3597.564, bsz=388.768, num_updates=1.68384e+06, lr=2.43697e-05, gnorm=3.467, clip=0.000, oom=0.000, wall=10442, train_wall=324858
| epoch 020:  53000 / 85886 loss=3.760, nll_loss=2.143, ppl=4.42, wps=18440, ups=5, wpb=3598.406, bsz=388.941, num_updates=1.68484e+06, lr=2.43625e-05, gnorm=3.466, clip=0.000, oom=0.000, wall=10639, train_wall=325051
| epoch 020:  54000 / 85886 loss=3.760, nll_loss=2.142, ppl=4.42, wps=18447, ups=5, wpb=3598.834, bsz=388.960, num_updates=1.68584e+06, lr=2.43552e-05, gnorm=3.465, clip=0.000, oom=0.000, wall=10831, train_wall=325240
| epoch 020:  55000 / 85886 loss=3.759, nll_loss=2.142, ppl=4.41, wps=18438, ups=5, wpb=3598.747, bsz=388.861, num_updates=1.68684e+06, lr=2.4348e-05, gnorm=3.465, clip=0.000, oom=0.000, wall=11031, train_wall=325435
| epoch 020:  56000 / 85886 loss=3.760, nll_loss=2.142, ppl=4.41, wps=18435, ups=5, wpb=3598.650, bsz=388.724, num_updates=1.68784e+06, lr=2.43408e-05, gnorm=3.466, clip=0.000, oom=0.000, wall=11228, train_wall=325628
| epoch 020:  57000 / 85886 loss=3.760, nll_loss=2.142, ppl=4.41, wps=18433, ups=5, wpb=3598.045, bsz=388.615, num_updates=1.68884e+06, lr=2.43336e-05, gnorm=3.468, clip=0.000, oom=0.000, wall=11423, train_wall=325819
| epoch 020:  58000 / 85886 loss=3.759, nll_loss=2.142, ppl=4.41, wps=18431, ups=5, wpb=3598.046, bsz=388.665, num_updates=1.68984e+06, lr=2.43264e-05, gnorm=3.468, clip=0.000, oom=0.000, wall=11619, train_wall=326011
| epoch 020:  59000 / 85886 loss=3.759, nll_loss=2.142, ppl=4.41, wps=18429, ups=5, wpb=3598.414, bsz=388.713, num_updates=1.69084e+06, lr=2.43192e-05, gnorm=3.468, clip=0.000, oom=0.000, wall=11817, train_wall=326204
| epoch 020:  60000 / 85886 loss=3.759, nll_loss=2.142, ppl=4.41, wps=18435, ups=5, wpb=3598.385, bsz=388.791, num_updates=1.69184e+06, lr=2.4312e-05, gnorm=3.468, clip=0.000, oom=0.000, wall=12008, train_wall=326392
| epoch 020:  61000 / 85886 loss=3.759, nll_loss=2.142, ppl=4.41, wps=18443, ups=5, wpb=3599.157, bsz=388.792, num_updates=1.69284e+06, lr=2.43048e-05, gnorm=3.468, clip=0.000, oom=0.000, wall=12201, train_wall=326581
| epoch 020:  62000 / 85886 loss=3.759, nll_loss=2.142, ppl=4.41, wps=18446, ups=5, wpb=3598.839, bsz=388.849, num_updates=1.69384e+06, lr=2.42977e-05, gnorm=3.469, clip=0.000, oom=0.000, wall=12393, train_wall=326769
| epoch 020:  63000 / 85886 loss=3.760, nll_loss=2.143, ppl=4.42, wps=18443, ups=5, wpb=3598.259, bsz=388.845, num_updates=1.69484e+06, lr=2.42905e-05, gnorm=3.470, clip=0.000, oom=0.000, wall=12588, train_wall=326960
| epoch 020:  64000 / 85886 loss=3.760, nll_loss=2.142, ppl=4.41, wps=18447, ups=5, wpb=3598.089, bsz=388.757, num_updates=1.69584e+06, lr=2.42833e-05, gnorm=3.470, clip=0.000, oom=0.000, wall=12780, train_wall=327148
| epoch 020:  65000 / 85886 loss=3.760, nll_loss=2.142, ppl=4.41, wps=18451, ups=5, wpb=3597.811, bsz=388.668, num_updates=1.69684e+06, lr=2.42762e-05, gnorm=3.470, clip=0.000, oom=0.000, wall=12971, train_wall=327335
| epoch 020:  66000 / 85886 loss=3.760, nll_loss=2.142, ppl=4.41, wps=18436, ups=5, wpb=3598.175, bsz=388.671, num_updates=1.69784e+06, lr=2.4269e-05, gnorm=3.470, clip=0.000, oom=0.000, wall=13178, train_wall=327538
| epoch 020:  67000 / 85886 loss=3.760, nll_loss=2.143, ppl=4.42, wps=18419, ups=5, wpb=3598.232, bsz=388.584, num_updates=1.69884e+06, lr=2.42619e-05, gnorm=3.471, clip=0.000, oom=0.000, wall=13385, train_wall=327741
| epoch 020:  68000 / 85886 loss=3.760, nll_loss=2.143, ppl=4.42, wps=18415, ups=5, wpb=3597.911, bsz=388.649, num_updates=1.69984e+06, lr=2.42547e-05, gnorm=3.472, clip=0.000, oom=0.000, wall=13582, train_wall=327933
| epoch 020:  69000 / 85886 loss=3.761, nll_loss=2.143, ppl=4.42, wps=18398, ups=5, wpb=3597.683, bsz=388.642, num_updates=1.70084e+06, lr=2.42476e-05, gnorm=3.472, clip=0.000, oom=0.000, wall=13789, train_wall=328136
| epoch 020:  70000 / 85886 loss=3.760, nll_loss=2.143, ppl=4.42, wps=18399, ups=5, wpb=3597.665, bsz=388.583, num_updates=1.70184e+06, lr=2.42405e-05, gnorm=3.472, clip=0.000, oom=0.000, wall=13984, train_wall=328327
| epoch 020:  71000 / 85886 loss=3.761, nll_loss=2.143, ppl=4.42, wps=18388, ups=5, wpb=3597.641, bsz=388.588, num_updates=1.70284e+06, lr=2.42334e-05, gnorm=3.472, clip=0.000, oom=0.000, wall=14187, train_wall=328526
| epoch 020:  72000 / 85886 loss=3.761, nll_loss=2.143, ppl=4.42, wps=18388, ups=5, wpb=3597.566, bsz=388.522, num_updates=1.70384e+06, lr=2.42263e-05, gnorm=3.473, clip=0.000, oom=0.000, wall=14383, train_wall=328717
| epoch 020:  73000 / 85886 loss=3.761, nll_loss=2.144, ppl=4.42, wps=18392, ups=5, wpb=3597.410, bsz=388.510, num_updates=1.70484e+06, lr=2.42191e-05, gnorm=3.473, clip=0.000, oom=0.000, wall=14575, train_wall=328905
| epoch 020:  74000 / 85886 loss=3.761, nll_loss=2.144, ppl=4.42, wps=18378, ups=5, wpb=3598.025, bsz=388.412, num_updates=1.70584e+06, lr=2.4212e-05, gnorm=3.473, clip=0.000, oom=0.000, wall=14785, train_wall=329110
| epoch 020:  75000 / 85886 loss=3.762, nll_loss=2.145, ppl=4.42, wps=18382, ups=5, wpb=3598.227, bsz=388.366, num_updates=1.70684e+06, lr=2.4205e-05, gnorm=3.473, clip=0.000, oom=0.000, wall=14978, train_wall=329299
| epoch 020:  76000 / 85886 loss=3.762, nll_loss=2.145, ppl=4.42, wps=18388, ups=5, wpb=3598.531, bsz=388.316, num_updates=1.70784e+06, lr=2.41979e-05, gnorm=3.472, clip=0.000, oom=0.000, wall=15170, train_wall=329487
| epoch 020:  77000 / 85886 loss=3.762, nll_loss=2.145, ppl=4.42, wps=18391, ups=5, wpb=3598.722, bsz=388.364, num_updates=1.70884e+06, lr=2.41908e-05, gnorm=3.473, clip=0.000, oom=0.000, wall=15364, train_wall=329677
| epoch 020:  78000 / 85886 loss=3.761, nll_loss=2.144, ppl=4.42, wps=18392, ups=5, wpb=3598.742, bsz=388.593, num_updates=1.70984e+06, lr=2.41837e-05, gnorm=3.474, clip=0.000, oom=0.000, wall=15559, train_wall=329868
| epoch 020:  79000 / 85886 loss=3.761, nll_loss=2.144, ppl=4.42, wps=18397, ups=5, wpb=3598.945, bsz=388.675, num_updates=1.71084e+06, lr=2.41766e-05, gnorm=3.475, clip=0.000, oom=0.000, wall=15751, train_wall=330056
| epoch 020:  80000 / 85886 loss=3.762, nll_loss=2.145, ppl=4.42, wps=18395, ups=5, wpb=3598.737, bsz=388.450, num_updates=1.71184e+06, lr=2.41696e-05, gnorm=3.475, clip=0.000, oom=0.000, wall=15947, train_wall=330248
| epoch 020:  81000 / 85886 loss=3.762, nll_loss=2.145, ppl=4.42, wps=18397, ups=5, wpb=3598.030, bsz=388.406, num_updates=1.71284e+06, lr=2.41625e-05, gnorm=3.476, clip=0.000, oom=0.000, wall=16138, train_wall=330435
| epoch 020:  82000 / 85886 loss=3.762, nll_loss=2.145, ppl=4.42, wps=18399, ups=5, wpb=3598.079, bsz=388.460, num_updates=1.71384e+06, lr=2.41555e-05, gnorm=3.475, clip=0.000, oom=0.000, wall=16332, train_wall=330625
| epoch 020:  83000 / 85886 loss=3.761, nll_loss=2.144, ppl=4.42, wps=18403, ups=5, wpb=3598.307, bsz=388.626, num_updates=1.71484e+06, lr=2.41484e-05, gnorm=3.475, clip=0.000, oom=0.000, wall=16525, train_wall=330814
| epoch 020:  84000 / 85886 loss=3.761, nll_loss=2.144, ppl=4.42, wps=18403, ups=5, wpb=3598.146, bsz=388.729, num_updates=1.71584e+06, lr=2.41414e-05, gnorm=3.476, clip=0.000, oom=0.000, wall=16720, train_wall=331005
| epoch 020:  85000 / 85886 loss=3.761, nll_loss=2.144, ppl=4.42, wps=18394, ups=5, wpb=3598.358, bsz=388.636, num_updates=1.71684e+06, lr=2.41344e-05, gnorm=3.477, clip=0.000, oom=0.000, wall=16925, train_wall=331205
| epoch 020 | loss 3.761 | nll_loss 2.144 | ppl 4.42 | wps 18391 | ups 5 | wpb 3598.518 | bsz 388.608 | num_updates 1.71772e+06 | lr 2.41281e-05 | gnorm 3.477 | clip 0.000 | oom 0.000 | wall 17102 | train_wall 331378
| epoch 020 | valid on 'valid' subset | loss 3.691 | nll_loss 1.968 | ppl 3.91 | num_updates 1.71772e+06 | best_loss 3.68737
| saved checkpoint ../models/pt/checkpoint20.pt (epoch 20 @ 1717720 updates) (writing took 6.6201276779174805 seconds)
| done training in 16813.3 seconds
