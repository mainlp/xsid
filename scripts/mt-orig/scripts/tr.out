Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer_wmt_en_de', attention_dropout=0.0, best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='../preprocessed/tr', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layer_wise_attention=False, layernorm_embedding=False, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format='simple', log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=20, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='../models/tr/checkpoint_last.pt', save_dir='../models/tr', save_interval=1, save_interval_updates=0, seed=1111, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0)
| [en] dictionary: 32000 types
| [tr] dictionary: 32000 types
| loaded 1958 examples from: ../preprocessed/tr/valid.en-tr.en
| loaded 1958 examples from: ../preprocessed/tr/valid.en-tr.tr
| ../preprocessed/tr valid en-tr 1958 examples
TransformerModel(
  (encoder): TransformerEncoder(
    (embed_tokens): Embedding(32000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (embed_tokens): Embedding(32000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
| model transformer_wmt_en_de, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 93290496 (num. trained: 93290496)
| training on 1 GPUs
| max tokens per GPU = 4096 and max sentences per GPU = None
| NOTICE: your device may support faster training with --fp16
| loaded checkpoint ../models/tr/checkpoint_last.pt (epoch 19 @ 2159369 updates)
| loading train data for epoch 19
| loaded 45788547 examples from: ../preprocessed/tr/train.en-tr.en
| loaded 45788547 examples from: ../preprocessed/tr/train.en-tr.tr
| ../preprocessed/tr train en-tr 45788547 examples
| WARNING: 3 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[35101288, 45099682, 21783171]
/home/robv/nlu-mt/fairseq/fairseq/optim/adam.py:160: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
| epoch 020:   1000 / 113651 loss=4.258, nll_loss=2.689, ppl=6.45, wps=16336, ups=5, wpb=3354.390, bsz=405.706, num_updates=2.16037e+06, lr=2.15147e-05, gnorm=4.833, clip=0.000, oom=0.000, wall=634, train_wall=410684
| epoch 020:   2000 / 113651 loss=4.269, nll_loss=2.702, ppl=6.51, wps=16938, ups=5, wpb=3364.246, bsz=406.573, num_updates=2.16137e+06, lr=2.15098e-05, gnorm=4.868, clip=0.001, oom=0.000, wall=826, train_wall=410872
| epoch 020:   3000 / 113651 loss=4.284, nll_loss=2.718, ppl=6.58, wps=17176, ups=5, wpb=3373.396, bsz=402.194, num_updates=2.16237e+06, lr=2.15048e-05, gnorm=4.850, clip=0.001, oom=0.000, wall=1018, train_wall=411059
| epoch 020:   4000 / 113651 loss=4.284, nll_loss=2.718, ppl=6.58, wps=17329, ups=5, wpb=3377.674, bsz=402.159, num_updates=2.16337e+06, lr=2.14998e-05, gnorm=4.842, clip=0.000, oom=0.000, wall=1208, train_wall=411245
| epoch 020:   5000 / 113651 loss=4.280, nll_loss=2.715, ppl=6.56, wps=17406, ups=5, wpb=3376.055, bsz=403.660, num_updates=2.16437e+06, lr=2.14948e-05, gnorm=4.870, clip=0.001, oom=0.000, wall=1398, train_wall=411431
| epoch 020:   6000 / 113651 loss=4.285, nll_loss=2.719, ppl=6.59, wps=17153, ups=5, wpb=3376.759, bsz=403.890, num_updates=2.16537e+06, lr=2.14899e-05, gnorm=4.882, clip=0.001, oom=0.000, wall=1610, train_wall=411636
| epoch 020:   7000 / 113651 loss=4.286, nll_loss=2.720, ppl=6.59, wps=17061, ups=5, wpb=3376.152, bsz=404.158, num_updates=2.16637e+06, lr=2.14849e-05, gnorm=4.884, clip=0.001, oom=0.000, wall=1814, train_wall=411836
| epoch 020:   8000 / 113651 loss=4.287, nll_loss=2.722, ppl=6.6, wps=16976, ups=5, wpb=3374.947, bsz=402.973, num_updates=2.16737e+06, lr=2.148e-05, gnorm=4.882, clip=0.000, oom=0.000, wall=2019, train_wall=412036
| epoch 020:   9000 / 113651 loss=4.290, nll_loss=2.726, ppl=6.61, wps=17056, ups=5, wpb=3373.322, bsz=401.721, num_updates=2.16837e+06, lr=2.1475e-05, gnorm=4.880, clip=0.000, oom=0.000, wall=2209, train_wall=412221
| epoch 020:  10000 / 113651 loss=4.291, nll_loss=2.726, ppl=6.62, wps=17094, ups=5, wpb=3368.666, bsz=400.746, num_updates=2.16937e+06, lr=2.14701e-05, gnorm=4.885, clip=0.000, oom=0.000, wall=2399, train_wall=412407
| epoch 020:  11000 / 113651 loss=4.288, nll_loss=2.723, ppl=6.6, wps=17039, ups=5, wpb=3368.388, bsz=402.083, num_updates=2.17037e+06, lr=2.14651e-05, gnorm=4.897, clip=0.001, oom=0.000, wall=2603, train_wall=412606
| epoch 020:  12000 / 113651 loss=4.287, nll_loss=2.722, ppl=6.6, wps=17076, ups=5, wpb=3370.259, bsz=402.797, num_updates=2.17137e+06, lr=2.14602e-05, gnorm=4.890, clip=0.001, oom=0.000, wall=2797, train_wall=412795
| epoch 020:  13000 / 113651 loss=4.289, nll_loss=2.724, ppl=6.61, wps=17093, ups=5, wpb=3370.741, bsz=401.970, num_updates=2.17237e+06, lr=2.14552e-05, gnorm=4.884, clip=0.001, oom=0.000, wall=2992, train_wall=412986
| epoch 020:  14000 / 113651 loss=4.289, nll_loss=2.724, ppl=6.61, wps=17093, ups=5, wpb=3373.334, bsz=401.712, num_updates=2.17337e+06, lr=2.14503e-05, gnorm=4.877, clip=0.000, oom=0.000, wall=3192, train_wall=413180
| epoch 020:  15000 / 113651 loss=4.289, nll_loss=2.724, ppl=6.61, wps=17090, ups=5, wpb=3371.715, bsz=401.911, num_updates=2.17437e+06, lr=2.14454e-05, gnorm=4.880, clip=0.001, oom=0.000, wall=3388, train_wall=413372
| epoch 020:  16000 / 113651 loss=4.289, nll_loss=2.724, ppl=6.61, wps=17082, ups=5, wpb=3374.145, bsz=402.332, num_updates=2.17537e+06, lr=2.14404e-05, gnorm=4.880, clip=0.000, oom=0.000, wall=3589, train_wall=413568
| epoch 020:  17000 / 113651 loss=4.291, nll_loss=2.727, ppl=6.62, wps=17114, ups=5, wpb=3372.384, bsz=401.893, num_updates=2.17637e+06, lr=2.14355e-05, gnorm=4.891, clip=0.001, oom=0.000, wall=3779, train_wall=413753
| epoch 020:  18000 / 113651 loss=4.294, nll_loss=2.729, ppl=6.63, wps=17135, ups=5, wpb=3373.725, bsz=401.982, num_updates=2.17737e+06, lr=2.14306e-05, gnorm=4.894, clip=0.001, oom=0.000, wall=3973, train_wall=413943
| epoch 020:  19000 / 113651 loss=4.294, nll_loss=2.730, ppl=6.63, wps=17133, ups=5, wpb=3374.139, bsz=401.974, num_updates=2.17837e+06, lr=2.14257e-05, gnorm=4.892, clip=0.001, oom=0.000, wall=4170, train_wall=414136
| epoch 020:  20000 / 113651 loss=4.294, nll_loss=2.730, ppl=6.63, wps=17138, ups=5, wpb=3375.730, bsz=401.872, num_updates=2.17937e+06, lr=2.14207e-05, gnorm=4.888, clip=0.001, oom=0.000, wall=4368, train_wall=414329
| epoch 020:  21000 / 113651 loss=4.294, nll_loss=2.729, ppl=6.63, wps=17150, ups=5, wpb=3377.125, bsz=401.598, num_updates=2.18037e+06, lr=2.14158e-05, gnorm=4.884, clip=0.001, oom=0.000, wall=4564, train_wall=414520
| epoch 020:  22000 / 113651 loss=4.294, nll_loss=2.729, ppl=6.63, wps=17170, ups=5, wpb=3376.095, bsz=401.537, num_updates=2.18137e+06, lr=2.14109e-05, gnorm=4.889, clip=0.001, oom=0.000, wall=4754, train_wall=414706
| epoch 020:  23000 / 113651 loss=4.294, nll_loss=2.730, ppl=6.63, wps=17189, ups=5, wpb=3376.577, bsz=401.619, num_updates=2.18237e+06, lr=2.1406e-05, gnorm=4.890, clip=0.001, oom=0.000, wall=4947, train_wall=414894
| epoch 020:  24000 / 113651 loss=4.296, nll_loss=2.732, ppl=6.64, wps=17199, ups=5, wpb=3377.456, bsz=401.407, num_updates=2.18337e+06, lr=2.14011e-05, gnorm=4.893, clip=0.001, oom=0.000, wall=5142, train_wall=415084
| epoch 020:  25000 / 113651 loss=4.294, nll_loss=2.730, ppl=6.63, wps=17195, ups=5, wpb=3378.207, bsz=402.043, num_updates=2.18437e+06, lr=2.13962e-05, gnorm=4.893, clip=0.001, oom=0.000, wall=5340, train_wall=415278
| epoch 020:  26000 / 113651 loss=4.294, nll_loss=2.730, ppl=6.64, wps=17185, ups=5, wpb=3378.526, bsz=402.165, num_updates=2.18537e+06, lr=2.13913e-05, gnorm=4.894, clip=0.001, oom=0.000, wall=5540, train_wall=415473
| epoch 020:  27000 / 113651 loss=4.295, nll_loss=2.731, ppl=6.64, wps=17184, ups=5, wpb=3377.558, bsz=402.288, num_updates=2.18637e+06, lr=2.13864e-05, gnorm=4.903, clip=0.001, oom=0.000, wall=5736, train_wall=415663
| epoch 020:  28000 / 113651 loss=4.295, nll_loss=2.730, ppl=6.64, wps=17198, ups=5, wpb=3378.441, bsz=402.580, num_updates=2.18737e+06, lr=2.13815e-05, gnorm=4.902, clip=0.001, oom=0.000, wall=5929, train_wall=415853
| epoch 020:  29000 / 113651 loss=4.295, nll_loss=2.731, ppl=6.64, wps=17206, ups=5, wpb=3378.768, bsz=402.801, num_updates=2.18837e+06, lr=2.13766e-05, gnorm=4.908, clip=0.001, oom=0.000, wall=6123, train_wall=416043
| epoch 020:  30000 / 113651 loss=4.296, nll_loss=2.732, ppl=6.64, wps=17203, ups=5, wpb=3378.659, bsz=402.728, num_updates=2.18937e+06, lr=2.13718e-05, gnorm=4.908, clip=0.001, oom=0.000, wall=6321, train_wall=416235
| epoch 020:  31000 / 113651 loss=4.296, nll_loss=2.732, ppl=6.64, wps=17197, ups=5, wpb=3378.761, bsz=402.563, num_updates=2.19037e+06, lr=2.13669e-05, gnorm=4.907, clip=0.001, oom=0.000, wall=6519, train_wall=416429
| epoch 020:  32000 / 113651 loss=4.297, nll_loss=2.733, ppl=6.65, wps=17212, ups=5, wpb=3378.229, bsz=402.239, num_updates=2.19137e+06, lr=2.1362e-05, gnorm=4.911, clip=0.001, oom=0.000, wall=6709, train_wall=416615
| epoch 020:  33000 / 113651 loss=4.296, nll_loss=2.732, ppl=6.64, wps=17209, ups=5, wpb=3378.800, bsz=402.578, num_updates=2.19237e+06, lr=2.13571e-05, gnorm=4.911, clip=0.001, oom=0.000, wall=6908, train_wall=416808
| epoch 020:  34000 / 113651 loss=4.295, nll_loss=2.731, ppl=6.64, wps=17204, ups=5, wpb=3379.282, bsz=403.365, num_updates=2.19337e+06, lr=2.13523e-05, gnorm=4.914, clip=0.001, oom=0.000, wall=7107, train_wall=417003
| epoch 020:  35000 / 113651 loss=4.295, nll_loss=2.731, ppl=6.64, wps=17196, ups=5, wpb=3378.015, bsz=403.396, num_updates=2.19437e+06, lr=2.13474e-05, gnorm=4.916, clip=0.001, oom=0.000, wall=7304, train_wall=417195
| epoch 020:  36000 / 113651 loss=4.296, nll_loss=2.732, ppl=6.64, wps=17199, ups=5, wpb=3378.262, bsz=403.347, num_updates=2.19537e+06, lr=2.13425e-05, gnorm=4.917, clip=0.001, oom=0.000, wall=7500, train_wall=417386
| epoch 020:  37000 / 113651 loss=4.296, nll_loss=2.732, ppl=6.64, wps=17214, ups=5, wpb=3378.587, bsz=403.668, num_updates=2.19637e+06, lr=2.13377e-05, gnorm=4.917, clip=0.001, oom=0.000, wall=7691, train_wall=417572
| epoch 020:  38000 / 113651 loss=4.296, nll_loss=2.732, ppl=6.64, wps=17227, ups=5, wpb=3378.265, bsz=403.641, num_updates=2.19737e+06, lr=2.13328e-05, gnorm=4.919, clip=0.001, oom=0.000, wall=7881, train_wall=417757
| epoch 020:  39000 / 113651 loss=4.296, nll_loss=2.733, ppl=6.65, wps=17244, ups=5, wpb=3379.179, bsz=403.693, num_updates=2.19837e+06, lr=2.1328e-05, gnorm=4.920, clip=0.001, oom=0.000, wall=8071, train_wall=417944
| epoch 020:  40000 / 113651 loss=4.297, nll_loss=2.733, ppl=6.65, wps=17259, ups=5, wpb=3379.840, bsz=403.701, num_updates=2.19937e+06, lr=2.13231e-05, gnorm=4.919, clip=0.001, oom=0.000, wall=8262, train_wall=418130
| epoch 020:  41000 / 113651 loss=4.296, nll_loss=2.733, ppl=6.65, wps=17248, ups=5, wpb=3378.756, bsz=403.750, num_updates=2.20037e+06, lr=2.13183e-05, gnorm=4.919, clip=0.001, oom=0.000, wall=8460, train_wall=418323
| epoch 020:  42000 / 113651 loss=4.296, nll_loss=2.732, ppl=6.64, wps=17253, ups=5, wpb=3379.883, bsz=403.856, num_updates=2.20137e+06, lr=2.13134e-05, gnorm=4.916, clip=0.001, oom=0.000, wall=8656, train_wall=418514
| epoch 020:  43000 / 113651 loss=4.296, nll_loss=2.732, ppl=6.64, wps=17255, ups=5, wpb=3379.462, bsz=403.752, num_updates=2.20237e+06, lr=2.13086e-05, gnorm=4.915, clip=0.001, oom=0.000, wall=8850, train_wall=418704
| epoch 020:  44000 / 113651 loss=4.296, nll_loss=2.733, ppl=6.65, wps=17254, ups=5, wpb=3379.183, bsz=403.564, num_updates=2.20337e+06, lr=2.13038e-05, gnorm=4.916, clip=0.001, oom=0.000, wall=9046, train_wall=418894
| epoch 020:  45000 / 113651 loss=4.297, nll_loss=2.733, ppl=6.65, wps=17257, ups=5, wpb=3379.081, bsz=403.377, num_updates=2.20437e+06, lr=2.12989e-05, gnorm=4.918, clip=0.001, oom=0.000, wall=9240, train_wall=419084
| epoch 020:  46000 / 113651 loss=4.297, nll_loss=2.733, ppl=6.65, wps=17250, ups=5, wpb=3379.785, bsz=403.395, num_updates=2.20537e+06, lr=2.12941e-05, gnorm=4.918, clip=0.001, oom=0.000, wall=9441, train_wall=419280
| epoch 020:  47000 / 113651 loss=4.297, nll_loss=2.733, ppl=6.65, wps=17253, ups=5, wpb=3379.580, bsz=403.409, num_updates=2.20637e+06, lr=2.12893e-05, gnorm=4.919, clip=0.001, oom=0.000, wall=9635, train_wall=419470
| epoch 020:  48000 / 113651 loss=4.297, nll_loss=2.733, ppl=6.65, wps=17246, ups=5, wpb=3378.469, bsz=403.393, num_updates=2.20737e+06, lr=2.12844e-05, gnorm=4.926, clip=0.001, oom=0.000, wall=9832, train_wall=419662
| epoch 020:  49000 / 113651 loss=4.297, nll_loss=2.733, ppl=6.65, wps=17251, ups=5, wpb=3379.026, bsz=403.335, num_updates=2.20837e+06, lr=2.12796e-05, gnorm=4.925, clip=0.001, oom=0.000, wall=10027, train_wall=419852
| epoch 020:  50000 / 113651 loss=4.297, nll_loss=2.734, ppl=6.65, wps=17263, ups=5, wpb=3379.720, bsz=403.401, num_updates=2.20937e+06, lr=2.12748e-05, gnorm=4.927, clip=0.001, oom=0.000, wall=10218, train_wall=420038
| epoch 020:  51000 / 113651 loss=4.297, nll_loss=2.734, ppl=6.65, wps=17267, ups=5, wpb=3379.527, bsz=403.342, num_updates=2.21037e+06, lr=2.127e-05, gnorm=4.929, clip=0.001, oom=0.000, wall=10411, train_wall=420226
| epoch 020:  52000 / 113651 loss=4.297, nll_loss=2.733, ppl=6.65, wps=17254, ups=5, wpb=3378.981, bsz=403.507, num_updates=2.21137e+06, lr=2.12652e-05, gnorm=4.929, clip=0.001, oom=0.000, wall=10612, train_wall=420423
| epoch 020:  53000 / 113651 loss=4.297, nll_loss=2.733, ppl=6.65, wps=17262, ups=5, wpb=3379.303, bsz=403.462, num_updates=2.21237e+06, lr=2.12604e-05, gnorm=4.929, clip=0.001, oom=0.000, wall=10804, train_wall=420611
| epoch 020:  54000 / 113651 loss=4.297, nll_loss=2.734, ppl=6.65, wps=17265, ups=5, wpb=3379.743, bsz=403.457, num_updates=2.21337e+06, lr=2.12556e-05, gnorm=4.929, clip=0.001, oom=0.000, wall=11000, train_wall=420802
| epoch 020:  55000 / 113651 loss=4.297, nll_loss=2.733, ppl=6.65, wps=17262, ups=5, wpb=3379.063, bsz=403.415, num_updates=2.21437e+06, lr=2.12508e-05, gnorm=4.930, clip=0.001, oom=0.000, wall=11195, train_wall=420992
| epoch 020:  56000 / 113651 loss=4.298, nll_loss=2.734, ppl=6.65, wps=17268, ups=5, wpb=3378.525, bsz=403.160, num_updates=2.21537e+06, lr=2.1246e-05, gnorm=4.935, clip=0.001, oom=0.000, wall=11385, train_wall=421178
| epoch 020:  57000 / 113651 loss=4.298, nll_loss=2.734, ppl=6.65, wps=17266, ups=5, wpb=3378.795, bsz=403.346, num_updates=2.21637e+06, lr=2.12412e-05, gnorm=4.935, clip=0.001, oom=0.000, wall=11583, train_wall=421371
| epoch 020:  58000 / 113651 loss=4.298, nll_loss=2.734, ppl=6.65, wps=17264, ups=5, wpb=3379.080, bsz=403.404, num_updates=2.21737e+06, lr=2.12364e-05, gnorm=4.936, clip=0.001, oom=0.000, wall=11781, train_wall=421564
| epoch 020:  59000 / 113651 loss=4.298, nll_loss=2.734, ppl=6.65, wps=17247, ups=5, wpb=3379.930, bsz=403.479, num_updates=2.21837e+06, lr=2.12316e-05, gnorm=4.935, clip=0.001, oom=0.000, wall=11991, train_wall=421769
| epoch 020:  60000 / 113651 loss=4.297, nll_loss=2.734, ppl=6.65, wps=17233, ups=5, wpb=3379.998, bsz=403.419, num_updates=2.21937e+06, lr=2.12268e-05, gnorm=4.934, clip=0.001, oom=0.000, wall=12197, train_wall=421969
| epoch 020:  61000 / 113651 loss=4.298, nll_loss=2.734, ppl=6.65, wps=17235, ups=5, wpb=3379.678, bsz=403.183, num_updates=2.22037e+06, lr=2.1222e-05, gnorm=4.934, clip=0.001, oom=0.000, wall=12390, train_wall=422159
| epoch 020:  62000 / 113651 loss=4.298, nll_loss=2.734, ppl=6.65, wps=17238, ups=5, wpb=3379.484, bsz=403.085, num_updates=2.22137e+06, lr=2.12173e-05, gnorm=4.936, clip=0.001, oom=0.000, wall=12583, train_wall=422347
| epoch 020:  63000 / 113651 loss=4.298, nll_loss=2.734, ppl=6.65, wps=17223, ups=5, wpb=3379.172, bsz=403.118, num_updates=2.22237e+06, lr=2.12125e-05, gnorm=4.938, clip=0.001, oom=0.000, wall=12789, train_wall=422547
| epoch 020:  64000 / 113651 loss=4.298, nll_loss=2.735, ppl=6.66, wps=17219, ups=5, wpb=3379.196, bsz=403.096, num_updates=2.22337e+06, lr=2.12077e-05, gnorm=4.937, clip=0.001, oom=0.000, wall=12988, train_wall=422742
| epoch 020:  65000 / 113651 loss=4.298, nll_loss=2.735, ppl=6.66, wps=17227, ups=5, wpb=3379.393, bsz=403.118, num_updates=2.22437e+06, lr=2.1203e-05, gnorm=4.938, clip=0.001, oom=0.000, wall=13180, train_wall=422929
| epoch 020:  66000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17234, ups=5, wpb=3379.518, bsz=402.962, num_updates=2.22537e+06, lr=2.11982e-05, gnorm=4.939, clip=0.001, oom=0.000, wall=13371, train_wall=423115
| epoch 020:  67000 / 113651 loss=4.299, nll_loss=2.736, ppl=6.66, wps=17234, ups=5, wpb=3379.180, bsz=402.851, num_updates=2.22637e+06, lr=2.11934e-05, gnorm=4.940, clip=0.001, oom=0.000, wall=13566, train_wall=423306
| epoch 020:  68000 / 113651 loss=4.299, nll_loss=2.736, ppl=6.66, wps=17241, ups=5, wpb=3378.954, bsz=402.901, num_updates=2.22737e+06, lr=2.11887e-05, gnorm=4.942, clip=0.001, oom=0.000, wall=13755, train_wall=423491
| epoch 020:  69000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17246, ups=5, wpb=3378.461, bsz=402.897, num_updates=2.22837e+06, lr=2.11839e-05, gnorm=4.943, clip=0.001, oom=0.000, wall=13945, train_wall=423676
| epoch 020:  70000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17253, ups=5, wpb=3378.159, bsz=402.978, num_updates=2.22937e+06, lr=2.11792e-05, gnorm=4.946, clip=0.001, oom=0.000, wall=14134, train_wall=423861
| epoch 020:  71000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17253, ups=5, wpb=3378.027, bsz=402.890, num_updates=2.23037e+06, lr=2.11744e-05, gnorm=4.947, clip=0.001, oom=0.000, wall=14330, train_wall=424052
| epoch 020:  72000 / 113651 loss=4.299, nll_loss=2.736, ppl=6.66, wps=17250, ups=5, wpb=3377.521, bsz=402.872, num_updates=2.23137e+06, lr=2.11697e-05, gnorm=4.949, clip=0.001, oom=0.000, wall=14526, train_wall=424243
| epoch 020:  73000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17253, ups=5, wpb=3377.434, bsz=402.827, num_updates=2.23237e+06, lr=2.11649e-05, gnorm=4.948, clip=0.001, oom=0.000, wall=14719, train_wall=424431
| epoch 020:  74000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17254, ups=5, wpb=3377.571, bsz=402.747, num_updates=2.23337e+06, lr=2.11602e-05, gnorm=4.947, clip=0.001, oom=0.000, wall=14914, train_wall=424622
| epoch 020:  75000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17244, ups=5, wpb=3377.779, bsz=402.749, num_updates=2.23437e+06, lr=2.11555e-05, gnorm=4.947, clip=0.001, oom=0.000, wall=15119, train_wall=424822
| epoch 020:  76000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17243, ups=5, wpb=3377.713, bsz=402.708, num_updates=2.23537e+06, lr=2.11507e-05, gnorm=4.948, clip=0.001, oom=0.000, wall=15316, train_wall=425014
| epoch 020:  77000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17244, ups=5, wpb=3377.521, bsz=402.680, num_updates=2.23637e+06, lr=2.1146e-05, gnorm=4.948, clip=0.001, oom=0.000, wall=15511, train_wall=425204
| epoch 020:  78000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17248, ups=5, wpb=3377.509, bsz=402.786, num_updates=2.23737e+06, lr=2.11413e-05, gnorm=4.948, clip=0.001, oom=0.000, wall=15703, train_wall=425392
| epoch 020:  79000 / 113651 loss=4.298, nll_loss=2.735, ppl=6.66, wps=17257, ups=5, wpb=3378.084, bsz=402.926, num_updates=2.23837e+06, lr=2.11365e-05, gnorm=4.948, clip=0.001, oom=0.000, wall=15893, train_wall=425578
| epoch 020:  80000 / 113651 loss=4.298, nll_loss=2.734, ppl=6.66, wps=17255, ups=5, wpb=3377.856, bsz=402.900, num_updates=2.23937e+06, lr=2.11318e-05, gnorm=4.949, clip=0.001, oom=0.000, wall=16089, train_wall=425769
| epoch 020:  81000 / 113651 loss=4.298, nll_loss=2.735, ppl=6.66, wps=17262, ups=5, wpb=3377.828, bsz=402.850, num_updates=2.24037e+06, lr=2.11271e-05, gnorm=4.950, clip=0.001, oom=0.000, wall=16279, train_wall=425954
| epoch 020:  82000 / 113651 loss=4.298, nll_loss=2.735, ppl=6.66, wps=17268, ups=5, wpb=3378.000, bsz=402.918, num_updates=2.24137e+06, lr=2.11224e-05, gnorm=4.950, clip=0.001, oom=0.000, wall=16469, train_wall=426140
| epoch 020:  83000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17275, ups=5, wpb=3378.005, bsz=402.909, num_updates=2.24237e+06, lr=2.11177e-05, gnorm=4.951, clip=0.001, oom=0.000, wall=16659, train_wall=426325
| epoch 020:  84000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17280, ups=5, wpb=3377.820, bsz=402.884, num_updates=2.24337e+06, lr=2.1113e-05, gnorm=4.952, clip=0.001, oom=0.000, wall=16848, train_wall=426510
| epoch 020:  85000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17285, ups=5, wpb=3377.582, bsz=402.937, num_updates=2.24437e+06, lr=2.11083e-05, gnorm=4.954, clip=0.001, oom=0.000, wall=17038, train_wall=426695
| epoch 020:  86000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17290, ups=5, wpb=3377.519, bsz=402.958, num_updates=2.24537e+06, lr=2.11036e-05, gnorm=4.953, clip=0.001, oom=0.000, wall=17228, train_wall=426880
| epoch 020:  87000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17285, ups=5, wpb=3377.854, bsz=402.901, num_updates=2.24637e+06, lr=2.10989e-05, gnorm=4.952, clip=0.001, oom=0.000, wall=17431, train_wall=427078
| epoch 020:  88000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17280, ups=5, wpb=3377.962, bsz=402.819, num_updates=2.24737e+06, lr=2.10942e-05, gnorm=4.951, clip=0.001, oom=0.000, wall=17632, train_wall=427275
| epoch 020:  89000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17278, ups=5, wpb=3378.128, bsz=402.835, num_updates=2.24837e+06, lr=2.10895e-05, gnorm=4.951, clip=0.001, oom=0.000, wall=17830, train_wall=427468
| epoch 020:  90000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17274, ups=5, wpb=3378.628, bsz=402.891, num_updates=2.24937e+06, lr=2.10848e-05, gnorm=4.951, clip=0.001, oom=0.000, wall=18031, train_wall=427664
| epoch 020:  91000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17277, ups=5, wpb=3378.704, bsz=402.859, num_updates=2.25037e+06, lr=2.10801e-05, gnorm=4.952, clip=0.001, oom=0.000, wall=18224, train_wall=427853
| epoch 020:  92000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17283, ups=5, wpb=3378.786, bsz=402.891, num_updates=2.25137e+06, lr=2.10754e-05, gnorm=4.952, clip=0.001, oom=0.000, wall=18414, train_wall=428038
| epoch 020:  93000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17287, ups=5, wpb=3378.747, bsz=402.879, num_updates=2.25237e+06, lr=2.10708e-05, gnorm=4.952, clip=0.001, oom=0.000, wall=18605, train_wall=428225
| epoch 020:  94000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17292, ups=5, wpb=3378.968, bsz=402.882, num_updates=2.25337e+06, lr=2.10661e-05, gnorm=4.952, clip=0.001, oom=0.000, wall=18797, train_wall=428412
| epoch 020:  95000 / 113651 loss=4.299, nll_loss=2.736, ppl=6.66, wps=17292, ups=5, wpb=3378.983, bsz=402.747, num_updates=2.25437e+06, lr=2.10614e-05, gnorm=4.952, clip=0.001, oom=0.000, wall=18992, train_wall=428602
| epoch 020:  96000 / 113651 loss=4.299, nll_loss=2.736, ppl=6.66, wps=17289, ups=5, wpb=3378.918, bsz=402.794, num_updates=2.25537e+06, lr=2.10567e-05, gnorm=4.953, clip=0.001, oom=0.000, wall=19191, train_wall=428796
| epoch 020:  97000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17293, ups=5, wpb=3378.681, bsz=402.803, num_updates=2.25637e+06, lr=2.10521e-05, gnorm=4.953, clip=0.001, oom=0.000, wall=19381, train_wall=428982
| epoch 020:  98000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17284, ups=5, wpb=3378.864, bsz=402.822, num_updates=2.25737e+06, lr=2.10474e-05, gnorm=4.953, clip=0.001, oom=0.000, wall=19587, train_wall=429182
| epoch 020:  99000 / 113651 loss=4.299, nll_loss=2.736, ppl=6.66, wps=17287, ups=5, wpb=3379.247, bsz=402.794, num_updates=2.25837e+06, lr=2.10427e-05, gnorm=4.953, clip=0.001, oom=0.000, wall=19781, train_wall=429373
| epoch 020:  100000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17282, ups=5, wpb=3379.041, bsz=402.735, num_updates=2.25937e+06, lr=2.10381e-05, gnorm=4.953, clip=0.001, oom=0.000, wall=19981, train_wall=429568
| epoch 020:  101000 / 113651 loss=4.299, nll_loss=2.736, ppl=6.66, wps=17281, ups=5, wpb=3378.837, bsz=402.690, num_updates=2.26037e+06, lr=2.10334e-05, gnorm=4.954, clip=0.001, oom=0.000, wall=20176, train_wall=429758
| epoch 020:  102000 / 113651 loss=4.299, nll_loss=2.736, ppl=6.66, wps=17284, ups=5, wpb=3379.118, bsz=402.735, num_updates=2.26137e+06, lr=2.10288e-05, gnorm=4.956, clip=0.001, oom=0.000, wall=20370, train_wall=429948
| epoch 020:  103000 / 113651 loss=4.299, nll_loss=2.736, ppl=6.66, wps=17286, ups=5, wpb=3379.161, bsz=402.868, num_updates=2.26237e+06, lr=2.10241e-05, gnorm=4.957, clip=0.001, oom=0.000, wall=20564, train_wall=430137
| epoch 020:  104000 / 113651 loss=4.299, nll_loss=2.735, ppl=6.66, wps=17288, ups=5, wpb=3379.147, bsz=402.978, num_updates=2.26337e+06, lr=2.10195e-05, gnorm=4.958, clip=0.001, oom=0.000, wall=20757, train_wall=430325
| epoch 020:  105000 / 113651 loss=4.299, nll_loss=2.736, ppl=6.66, wps=17289, ups=5, wpb=3378.869, bsz=402.899, num_updates=2.26437e+06, lr=2.10149e-05, gnorm=4.959, clip=0.001, oom=0.000, wall=20949, train_wall=430513
| epoch 020:  106000 / 113651 loss=4.300, nll_loss=2.737, ppl=6.67, wps=17288, ups=5, wpb=3378.936, bsz=402.716, num_updates=2.26537e+06, lr=2.10102e-05, gnorm=4.960, clip=0.001, oom=0.000, wall=21146, train_wall=430705
| epoch 020:  107000 / 113651 loss=4.300, nll_loss=2.737, ppl=6.67, wps=17291, ups=5, wpb=3378.939, bsz=402.740, num_updates=2.26637e+06, lr=2.10056e-05, gnorm=4.961, clip=0.001, oom=0.000, wall=21338, train_wall=430892
| epoch 020:  108000 / 113651 loss=4.300, nll_loss=2.737, ppl=6.67, wps=17295, ups=5, wpb=3378.708, bsz=402.684, num_updates=2.26737e+06, lr=2.10009e-05, gnorm=4.961, clip=0.001, oom=0.000, wall=21527, train_wall=431077
| epoch 020:  109000 / 113651 loss=4.300, nll_loss=2.736, ppl=6.66, wps=17290, ups=5, wpb=3379.103, bsz=402.830, num_updates=2.26837e+06, lr=2.09963e-05, gnorm=4.961, clip=0.001, oom=0.000, wall=21731, train_wall=431276
| epoch 020:  110000 / 113651 loss=4.300, nll_loss=2.736, ppl=6.66, wps=17295, ups=5, wpb=3379.130, bsz=402.873, num_updates=2.26937e+06, lr=2.09917e-05, gnorm=4.961, clip=0.001, oom=0.000, wall=21921, train_wall=431461
| epoch 020:  111000 / 113651 loss=4.300, nll_loss=2.736, ppl=6.66, wps=17286, ups=5, wpb=3379.135, bsz=402.915, num_updates=2.27037e+06, lr=2.09871e-05, gnorm=4.960, clip=0.001, oom=0.000, wall=22128, train_wall=431663
| epoch 020:  112000 / 113651 loss=4.299, nll_loss=2.736, ppl=6.66, wps=17274, ups=5, wpb=3378.756, bsz=402.893, num_updates=2.27137e+06, lr=2.09824e-05, gnorm=4.960, clip=0.001, oom=0.000, wall=22336, train_wall=431865
| epoch 020:  113000 / 113651 loss=4.299, nll_loss=2.736, ppl=6.66, wps=17272, ups=5, wpb=3378.469, bsz=402.933, num_updates=2.27237e+06, lr=2.09778e-05, gnorm=4.961, clip=0.001, oom=0.000, wall=22532, train_wall=432057
| epoch 020 | loss 4.299 | nll_loss 2.736 | ppl 6.66 | wps 17267 | ups 5 | wpb 3378.474 | bsz 402.887 | num_updates 2.27302e+06 | lr 2.09748e-05 | gnorm 4.960 | clip 0.001 | oom 0.000 | wall 22666 | train_wall 432187
| epoch 020 | valid on 'valid' subset | loss 4.808 | nll_loss 3.204 | ppl 9.21 | num_updates 2.27302e+06 | best_loss 4.78176
| saved checkpoint ../models/tr/checkpoint20.pt (epoch 20 @ 2273020 updates) (writing took 6.791621685028076 seconds)
| done training in 22245.4 seconds
